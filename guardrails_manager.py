"""
NeMo Guardrails integration for the AI Data Analysis Chatbot
"""
import os
import logging
from typing import Dict, Any, Optional
from nemoguardrails import LLMRails, RailsConfig
from nemoguardrails.actions import action

logger = logging.getLogger(__name__)

class GuardrailsManager:
    """
    Manages NeMo Guardrails integration for secure AI interactions
    """
    
    def __init__(self, config_path: str = "config"):
        """
        Initialize the guardrails manager
        
        Args:
            config_path: Path to the guardrails configuration directory
        """
        self.config_path = config_path
        self.rails = None
        self._initialize_rails()
    
    def _initialize_rails(self):
        """
        Initialize NeMo Guardrails with configuration
        """
        try:
            # For now, use our custom security validation without NeMo Guardrails LLM dependency
            # This provides rule-based security without requiring external LLM setup
            self.rails = "custom_security"  # Placeholder for custom implementation
            
            logger.info("Custom security rails initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize security rails: {e}")
            self.rails = None
    
    def _register_custom_actions(self):
        """
        Register custom actions for data analysis context
        """
        if not self.rails:
            return
        
        @action(is_system_action=True)
        async def validate_data_request(context: Optional[Dict[str, Any]] = None):
            """Validate data analysis requests for safety"""
            from config.actions import validate_data_request
            return await validate_data_request(context or {})
        
        @action(is_system_action=True)
        async def sanitize_code_output(context: Optional[Dict[str, Any]] = None):
            """Sanitize generated code before execution"""
            from config.actions import sanitize_code_output
            return await sanitize_code_output(context or {})
        
        @action(is_system_action=True)
        async def check_data_compliance(context: Optional[Dict[str, Any]] = None):
            """Ensure data processing complies with privacy regulations"""
            from config.actions import check_data_compliance
            return await check_data_compliance(context or {})
    
    async def process_input(self, user_message: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Process user input through custom security validation
        
        Args:
            user_message: The user's input message
            context: Additional context information
            
        Returns:
            Dict containing processed message and safety information
        """
        if not self.rails:
            logger.warning("Security rails not initialized, proceeding without protection")
            return {
                "message": user_message,
                "is_safe": True,
                "guardrails_active": False
            }
        
        try:
            # Custom security validation
            from config.actions import validate_data_request
            
            validation_result = await validate_data_request({
                "user_message": user_message,
                **(context or {})
            })
            
            if not validation_result.get("is_safe", True):
                return {
                    "message": user_message,
                    "is_safe": False,
                    "guardrails_active": True,
                    "blocked_reason": validation_result.get("reason", "security_risk"),
                    "guardrails_response": validation_result.get("message", "Request blocked for security reasons")
                }
            
            return {
                "message": user_message,
                "is_safe": True,
                "guardrails_active": True,
                "processed_message": user_message
            }
            
        except Exception as e:
            logger.error(f"Error processing input through security validation: {e}")
            return {
                "message": user_message,
                "is_safe": True,
                "guardrails_active": False,
                "error": str(e)
            }
    
    async def process_output(self, bot_response: str, generated_code: Optional[str] = None, 
                           context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Process bot output through custom security validation
        
        Args:
            bot_response: The bot's response
            generated_code: Any code generated by the bot
            context: Additional context information
            
        Returns:
            Dict containing processed output and safety information
        """
        if not self.rails:
            logger.warning("Security rails not initialized, proceeding without protection")
            return {
                "response": bot_response,
                "code": generated_code,
                "is_safe": True,
                "guardrails_active": False
            }
        
        try:
            # Process code if present
            processed_code = generated_code
            if generated_code:
                code_result = await self._validate_code_safety(generated_code, context or {})
                processed_code = code_result.get("sanitized_code", generated_code)
                
                if not code_result.get("is_safe", True):
                    return {
                        "response": code_result.get("message", bot_response),
                        "code": processed_code,
                        "is_safe": False,
                        "guardrails_active": True,
                        "blocked_reason": "code_safety"
                    }
            
            return {
                "response": bot_response,
                "code": processed_code,
                "is_safe": True,
                "guardrails_active": True
            }
            
        except Exception as e:
            logger.error(f"Error processing output through security validation: {e}")
            return {
                "response": bot_response,
                "code": generated_code,
                "is_safe": True,
                "guardrails_active": False,
                "error": str(e)
            }
    
    async def _validate_code_safety(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate code safety using custom actions
        """
        try:
            from config.actions import sanitize_code_output
            context["generated_code"] = code
            return await sanitize_code_output(context)
        except Exception as e:
            logger.error(f"Error validating code safety: {e}")
            return {"is_safe": True, "sanitized_code": code}
    
    def is_active(self) -> bool:
        """
        Check if guardrails are active and functioning
        """
        return self.rails is not None
    
    def get_status(self) -> Dict[str, Any]:
        """
        Get guardrails status information
        """
        return {
            "active": self.is_active(),
            "config_path": self.config_path,
            "version": "nemoguardrails>=0.8.0"
        }

# Global instance
guardrails_manager = None

def get_guardrails_manager() -> GuardrailsManager:
    """
    Get the global guardrails manager instance
    """
    global guardrails_manager
    if guardrails_manager is None:
        config_path = os.path.join(os.path.dirname(__file__), "config")
        guardrails_manager = GuardrailsManager(config_path)
    return guardrails_manager